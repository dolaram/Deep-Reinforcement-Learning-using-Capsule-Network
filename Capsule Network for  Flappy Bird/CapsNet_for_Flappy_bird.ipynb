{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPSULE NETWORK FOR REINFORCEMENT LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sys\n",
    "import time \n",
    "sys.path.append(\"game/\")\n",
    "import wrapped_flappy_bird as game\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import deque\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the RL environment Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-9\n",
    "iter_routing = 2\n",
    "train_freq = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME = 'bird' # the name of the game being played for log files\n",
    "ACTIONS = 2 # number of valid actions\n",
    "GAMMA = 0.99 # decay rate of past observations\n",
    "OBSERVE = 1000 # timesteps to observe before training\n",
    "EXPLORE = 1000 # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Capsule Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(vector):\n",
    "    vec_squared_norm = reduce_sum(tf.square(vector), -2, keepdims=True)\n",
    "    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + epsilon)\n",
    "    vec_squashed = scalar_factor * vector  # element-wise\n",
    "    return(vec_squashed)\n",
    "def routing(input, b_IJ):\n",
    "    # W: [1, num_caps_i, num_caps_j * len_v_j, len_u_j, 1]\n",
    "    W = tf.get_variable('Weight', shape=(1, 1024, 160, 8, 1), dtype=tf.float32,\n",
    "                        initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "    biases = tf.get_variable('bias', shape=(1, 1, 10, 16, 1))\n",
    "    # A better solution is using element-wise multiply, reduce_sum and reshape\n",
    "    # ops instead. Matmul [a, b] x [b, c] is equal to a series ops as\n",
    "    # element-wise multiply [a*c, b] * [a*c, b], reduce_sum at axis=1 and\n",
    "    # reshape to [a, c]\n",
    "    input = tf.tile(input, [1, 1, 160, 1, 1])\n",
    "    #assert input.get_shape() == [cfg.batch_size, 1024, 160, 8, 1]\n",
    "\n",
    "    u_hat = reduce_sum(W * input, axis=3, keepdims=True)\n",
    "    u_hat = tf.reshape(u_hat, shape=[-1, 1024, 10, 16, 1])\n",
    "    #assert u_hat.get_shape() == [cfg.batch_size, 1024, 10, 16, 1]\n",
    "\n",
    "    # In forward, u_hat_stopped = u_hat; in backward, no gradient passed back from u_hat_stopped to u_hat\n",
    "    u_hat_stopped = tf.stop_gradient(u_hat, name='stop_gradient')\n",
    "\n",
    "    # line 3,for r iterations do\n",
    "    for r_iter in range(iter_routing):\n",
    "        with tf.variable_scope('iter_' + str(r_iter)):\n",
    "            # line 4:\n",
    "            # => [batch_size, 1024, 10, 1, 1]\n",
    "            c_IJ = softmax(b_IJ, axis=2)\n",
    "\n",
    "            # At last iteration, use `u_hat` in order to receive gradients from the following graph\n",
    "            if r_iter == iter_routing - 1:\n",
    "                # line 5:\n",
    "                # weighting u_hat with c_IJ, element-wise in the last two dims\n",
    "                # => [batch_size, 1024, 10, 16, 1]\n",
    "                s_J = tf.multiply(c_IJ, u_hat)\n",
    "                # then sum in the second dim, resulting in [batch_size, 1, 10, 16, 1]\n",
    "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
    "                #assert s_J.get_shape() == [cfg.batch_size, 1, 10, 16, 1]\n",
    "\n",
    "                # line 6:\n",
    "                # squash using Eq.1,\n",
    "                v_J = squash(s_J)\n",
    "                #assert v_J.get_shape() == [cfg.batch_size, 1, 10, 16, 1]\n",
    "            elif r_iter < iter_routing - 1:  # Inner iterations, do not apply backpropagation\n",
    "                s_J = tf.multiply(c_IJ, u_hat_stopped)\n",
    "                s_J = reduce_sum(s_J, axis=1, keepdims=True) + biases\n",
    "                v_J = squash(s_J)\n",
    "\n",
    "                # line 7:\n",
    "                # reshape & tile v_j from [batch_size ,1, 10, 16, 1] to [batch_size, 1024, 10, 16, 1]\n",
    "                # then matmul in the last tow dim: [16, 1].T x [16, 1] => [1, 1], reduce mean in the\n",
    "                # batch_size dim, resulting in [1, 1024, 10, 1, 1]\n",
    "                v_J_tiled = tf.tile(v_J, [1, 1024, 1, 1, 1])\n",
    "                u_produce_v = reduce_sum(u_hat_stopped * v_J_tiled, axis=3, keepdims=True)\n",
    "                #assert u_produce_v.get_shape() == [cfg.batch_size, 1024, 10, 1, 1]\n",
    "\n",
    "                # b_IJ += tf.reduce_sum(u_produce_v, axis=0, keep_dims=True)\n",
    "                b_IJ += u_produce_v\n",
    "    return(v_J)\n",
    "# For version compatibility\n",
    "def reduce_sum(input_tensor, axis=None, keepdims=False):\n",
    "    return tf.reduce_sum(input_tensor, axis=axis, keepdims=keepdims)\n",
    "# For version compatibility\n",
    "def softmax(logits, axis=None):\n",
    "    return tf.nn.softmax(logits, axis=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified DEEP Q-Capsule Network (DQCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNetwork():\n",
    "    # input layer\n",
    "    s= tf.placeholder(\"float\", [None, 84, 84, 4])\n",
    "    coeff = tf.placeholder(tf.float32, shape=(None, 1024, 10, 1, 1))\n",
    "    ####################### New Network COnfiguration #####################    \n",
    "    w_initializer, b_initializer = tf.random_normal_initializer(0., 0.01), tf.constant_initializer(0.01)\n",
    "    w1 = tf.get_variable('w1',[8, 8, 4, 64],initializer=w_initializer)\n",
    "    b1 = tf.get_variable('b1',[64],initializer=b_initializer)\n",
    "    # Convolution Layer\n",
    "    # Conv1, [batch_size, 20, 20, 64]\n",
    "    l1 = tf.nn.conv2d(s, w1, strides=[1, 4, 4, 1], padding=\"VALID\")\n",
    "    \n",
    "    conv1 = tf.nn.relu(tf.nn.bias_add(l1, b1))\n",
    "    \n",
    "    conv1 = tf.reshape(conv1,[-1,20,20,64])\n",
    "    \n",
    "    capsules = tf.contrib.layers.conv2d(conv1, 16 * 8, kernel_size=6, stride=2, padding=\"VALID\",\n",
    "                    activation_fn = tf.nn.relu,\n",
    "                    weights_initializer = tf.contrib.layers.xavier_initializer(uniform=False),\n",
    "                    biases_initializer=tf.constant_initializer(0))\n",
    "    \n",
    "    capsules = tf.reshape(capsules, (-1, 1024, 8, 1)) #Reshape to(batch_szie, 1152, 8, 1)\n",
    "    \n",
    "    capsules = squash(capsules)\n",
    "    \n",
    "    input_caps2 = tf.reshape(capsules, shape=(-1, 1024, 1, capsules.shape[-2].value, 1))\n",
    "    \n",
    "    caps2 = routing(input_caps2, coeff)\n",
    "    \n",
    "    vector_j = tf.reshape(caps2, shape=(-1, 160))\n",
    "    #print(vector_j)\n",
    "    q_eval = tf.contrib.layers.fully_connected(vector_j, num_outputs=ACTIONS, activation_fn=None)\n",
    "\n",
    "    #print(q_eval)\n",
    "    readout = q_eval\n",
    "    return s, coeff, readout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for training of CapsNet Agent for Flappy bird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork(s, coeff, readout, sess):\n",
    "    tick = time.time()\n",
    "    # define the cost function\n",
    "    a = tf.placeholder(\"float\", [None, ACTIONS])\n",
    "    y = tf.placeholder(\"float\", [None])\n",
    "    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices = 1)\n",
    "    cost = tf.reduce_mean(tf.square(y - readout_action))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cost)\n",
    "\n",
    "    # open up a game state to communicate with emulator\n",
    "    game_state = game.GameState()\n",
    "    \n",
    "    # store the previous observations in replay memory\n",
    "    D = deque()\n",
    "    \"\"\"\n",
    "    with open(\"my_saved_queue.obj\",\"rb\") as queue_save_file:\n",
    "        D = pickle.load(queue_save_file)\"\"\"\n",
    "    # get the first state by doing nothing and preprocess the image to 84x84x4\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] = 1\n",
    "    x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
    "    x_t = cv2.cvtColor(cv2.resize(x_t[:,:-110,:], (84, 84)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # saving and loading networks\n",
    "    saver = tf.train.Saver()\n",
    "    checkpoint = tf.train.get_checkpoint_state(\"saved_networks\")\n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Could not find old network weights\")\n",
    "        \n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    b_IJ1 = np.zeros((1, 1024, 10, 1, 1)).astype(np.float32) # batch_size=1\n",
    "    b_IJ2 = np.zeros((BATCH, 1024, 10, 1, 1)).astype(np.float32) # batch_size=BATCH\n",
    "    epsilon = 0.0005#INITIAL_EPSILON\n",
    "    t = 320000\n",
    "    pscore = 0\n",
    "    episode = 0\n",
    "    loss = 0\n",
    "    Q_MAX = -1100000\n",
    "    tick = time.time()\n",
    "    action_freq = np.zeros(ACTIONS)\n",
    "    while True:\n",
    "        # choose an action epsilon greedily\n",
    "        # readout_t = readout.eval(feed_dict = {s : [s_t].reshape((1,80,80,4))})[0]\n",
    "        \n",
    "        readout_t = readout.eval(feed_dict = {s:s_t.reshape((1,84,84,4)), coeff:b_IJ1})\n",
    "        \n",
    "        a_t = np.zeros([ACTIONS])\n",
    "        action_index = 0\n",
    "        if random.random() <= epsilon or t <= OBSERVE:\n",
    "            action_index = random.randrange(ACTIONS)\n",
    "            a_t[action_index] = 1\n",
    "        else:\n",
    "            action_index = np.argmax(readout_t)\n",
    "            a_t[action_index] = 1\n",
    "\n",
    "        # scale down epsilon\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "        # run the selected action and observe next state and reward\n",
    "        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n",
    "        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored[:,:-110,:], (84, 84)), cv2.COLOR_BGR2GRAY)\n",
    "        ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)\n",
    "        x_t1 = np.reshape(x_t1, (84, 84, 1))\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)\n",
    "        action_freq += a_t\n",
    "        # store the transition in D\n",
    "        D.append((s_t, a_t, r_t, s_t1, terminal))\n",
    "        if len(D) > REPLAY_MEMORY:\n",
    "            D.popleft()\n",
    "        \n",
    "        # only train if done observing\n",
    "        if t > (320000 + 100) and t%train_freq==0:\n",
    "            # sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "\n",
    "            # get the batch variables\n",
    "            s_j_batch = [d[0] for d in minibatch]\n",
    "            a_batch = [d[1] for d in minibatch]\n",
    "            r_batch = [d[2] for d in minibatch]\n",
    "            s_j1_batch = [d[3] for d in minibatch]\n",
    "\n",
    "            y_batch = []\n",
    "            readout_j1_batch = readout.eval(feed_dict = {s:s_j1_batch, coeff:b_IJ2 })\n",
    "            #readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})\n",
    "            for i in range(0, len(minibatch)):\n",
    "                # if terminal only equals reward\n",
    "                if minibatch[i][4]:\n",
    "                    y_batch.append(r_batch[i])\n",
    "                else:\n",
    "                    y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
    "\n",
    "            # perform gradient step\n",
    "            train_step.run(feed_dict = {\n",
    "                y : y_batch,\n",
    "                a : a_batch,\n",
    "                s : s_j_batch,\n",
    "                coeff: b_IJ2})\n",
    "            loss = cost.eval(feed_dict = {\n",
    "                y : y_batch,\n",
    "                a : a_batch,\n",
    "                s : s_j_batch,\n",
    "                coeff: b_IJ2})\n",
    "\n",
    "        # update the old values\n",
    "        s_t = s_t1\n",
    "        t += 1\n",
    "        if(Q_MAX < np.max(readout_t) ):\n",
    "            Q_MAX = np.max(readout_t)\n",
    "        # save progress every 10000 iterations\n",
    "#         if t % 10000 == 0:\n",
    "#             saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)\n",
    "#             with open(\"my_saved_queue.obj\",\"wb+\") as queue_save_file:\n",
    "#                 pickle.dump(D, queue_save_file)\n",
    "\n",
    "        if(r_t==1):\n",
    "            pscore += 1\n",
    "            print(\"ts\", t,\"Q_MAX %e\" % Q_MAX,\"/e\", round(epsilon,3),\"/pscore\",pscore,\"/loss\",loss,\"/ Q_MAX %e\" % np.max(readout_t))\n",
    "        if(terminal):#and (pscore > 5)\n",
    "            print(\"ts\", t,\"Q_MAX %e\" % Q_MAX,\"/e\", round(epsilon,3),\"/pscore\",pscore,\"/loss\",loss,\"/ Q_MAX %e\" % np.max(readout_t))\n",
    "            pscore = 0\n",
    "        if(terminal == 1):\n",
    "            episode +=1\n",
    "            Q_MAX = -1100000\n",
    "            action_freq = np.zeros(ACTIONS)\n",
    "        if(pscore > 10000):\n",
    "            print(\"Game_Ends_in Time:\",int(time.time() - tick))\n",
    "            break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of DQCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved_networks/bird-dqn-1790000\n",
      "Successfully loaded: saved_networks/bird-dqn-1790000\n",
      "ts 320060 Q_MAX 1.330941e+01 /e 0.0 /pscore 1 /loss 0 / Q_MAX 1.330941e+01\n",
      "ts 320096 Q_MAX 1.330941e+01 /e 0.0 /pscore 2 /loss 0 / Q_MAX 1.329995e+01\n",
      "ts 320134 Q_MAX 1.331949e+01 /e 0.0 /pscore 3 /loss 0 / Q_MAX 1.331949e+01\n",
      "ts 320170 Q_MAX 1.332076e+01 /e 0.0 /pscore 4 /loss 0 / Q_MAX 1.331321e+01\n",
      "ts 320208 Q_MAX 1.336658e+01 /e 0.0 /pscore 5 /loss 0 / Q_MAX 1.336658e+01\n",
      "ts 320244 Q_MAX 1.337020e+01 /e 0.0 /pscore 6 /loss 0 / Q_MAX 1.337020e+01\n",
      "ts 320282 Q_MAX 1.339382e+01 /e 0.0 /pscore 7 /loss 0 / Q_MAX 1.339382e+01\n",
      "ts 320318 Q_MAX 1.339382e+01 /e 0.0 /pscore 8 /loss 0 / Q_MAX 1.335733e+01\n",
      "ts 320356 Q_MAX 1.339382e+01 /e 0.0 /pscore 9 /loss 0 / Q_MAX 1.338951e+01\n",
      "ts 320392 Q_MAX 1.344047e+01 /e 0.0 /pscore 10 /loss 0 / Q_MAX 1.344047e+01\n",
      "ts 320430 Q_MAX 1.344047e+01 /e 0.0 /pscore 11 /loss 0 / Q_MAX 1.330993e+01\n",
      "ts 320466 Q_MAX 1.344047e+01 /e 0.0 /pscore 12 /loss 0 / Q_MAX 1.328252e+01\n",
      "ts 320504 Q_MAX 1.344047e+01 /e 0.0 /pscore 13 /loss 0 / Q_MAX 1.333364e+01\n",
      "ts 320540 Q_MAX 1.344047e+01 /e 0.0 /pscore 14 /loss 0 / Q_MAX 1.331367e+01\n",
      "ts 320578 Q_MAX 1.344047e+01 /e 0.0 /pscore 15 /loss 0 / Q_MAX 1.332900e+01\n",
      "ts 320614 Q_MAX 1.344047e+01 /e 0.0 /pscore 16 /loss 0 / Q_MAX 1.330496e+01\n",
      "ts 320652 Q_MAX 1.344047e+01 /e 0.0 /pscore 17 /loss 0 / Q_MAX 1.328681e+01\n",
      "ts 320688 Q_MAX 1.344047e+01 /e 0.0 /pscore 18 /loss 0 / Q_MAX 1.338992e+01\n",
      "ts 320726 Q_MAX 1.344047e+01 /e 0.0 /pscore 19 /loss 0 / Q_MAX 1.332987e+01\n",
      "ts 320762 Q_MAX 1.344047e+01 /e 0.0 /pscore 20 /loss 0 / Q_MAX 1.337881e+01\n",
      "ts 320800 Q_MAX 1.344047e+01 /e 0.0 /pscore 21 /loss 0 / Q_MAX 1.334008e+01\n",
      "ts 320836 Q_MAX 1.344047e+01 /e 0.0 /pscore 22 /loss 0 / Q_MAX 1.337449e+01\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "s, coeff, readout = createNetwork()\n",
    "trainNetwork(s, coeff, readout, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
